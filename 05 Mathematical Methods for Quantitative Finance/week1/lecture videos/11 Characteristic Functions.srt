0
00:00:00,000 --> 00:00:01,970


1
00:00:01,970 --> 00:00:04,310
PROFESSOR: Let's look at a little bit more detail

2
00:00:04,310 --> 00:00:08,720
at generating functions and central limit theorem.

3
00:00:08,720 --> 00:00:11,258
We'll be doing this using Fourier transforms.

4
00:00:11,258 --> 00:00:13,050
So if you've never seen a Fourier transform

5
00:00:13,050 --> 00:00:16,160
with a complex analysis, you can either skip this part

6
00:00:16,160 --> 00:00:19,130
or just watch and take a look at it.

7
00:00:19,130 --> 00:00:20,780
This is background material, it's

8
00:00:20,780 --> 00:00:22,970
not essential for what we're going to be doing.

9
00:00:22,970 --> 00:00:25,520
But for people who have done this math and who

10
00:00:25,520 --> 00:00:27,950
are familiar with it, it's an interesting way

11
00:00:27,950 --> 00:00:31,670
to see how the central limit theorem emerges

12
00:00:31,670 --> 00:00:33,870
from some simple calculations.

13
00:00:33,870 --> 00:00:37,160
So you may have seen in a statistics or probability

14
00:00:37,160 --> 00:00:39,740
course, the moment generating function, that's

15
00:00:39,740 --> 00:00:43,730
the expectation of e to the X. And here, we're

16
00:00:43,730 --> 00:00:45,920
going to do a slight variation of it.

17
00:00:45,920 --> 00:00:48,770
We're going to introduce i, the square root of minus 1.

18
00:00:48,770 --> 00:00:51,152
So this is going to look like complex analysis.

19
00:00:51,152 --> 00:00:52,610
But don't worry, there won't really

20
00:00:52,610 --> 00:00:55,610
be any imaginary numbers in our financial market.

21
00:00:55,610 --> 00:00:58,070
The only imagination is, maybe, the riches

22
00:00:58,070 --> 00:01:00,680
that we would like to be thinking about getting.

23
00:01:00,680 --> 00:01:04,519
But everything else is going to be brutally real.

24
00:01:04,519 --> 00:01:06,590
Nevertheless, it's a good mathematical trick

25
00:01:06,590 --> 00:01:08,660
because Fourier transforms, as we'll see,

26
00:01:08,660 --> 00:01:10,040
have some nice properties.

27
00:01:10,040 --> 00:01:13,130
So the characteristic function serves the same role

28
00:01:13,130 --> 00:01:15,980
as the moment generating function.

29
00:01:15,980 --> 00:01:18,200
That is, it lets us summarize all

30
00:01:18,200 --> 00:01:20,100
of the moments of a distribution.

31
00:01:20,100 --> 00:01:22,480
So the idea is that we can describe the function

32
00:01:22,480 --> 00:01:26,870
by-- if we look at the expectation of e

33
00:01:26,870 --> 00:01:31,430
to the itX, where X is our random variable,

34
00:01:31,430 --> 00:01:34,970
and we expand out that exponential in a Taylor series,

35
00:01:34,970 --> 00:01:40,770
we get 1 plus itX plus 1/2 i squared t squared X squared,

36
00:01:40,770 --> 00:01:42,080
and so on.

37
00:01:42,080 --> 00:01:43,550
And then we apply linearity, and we

38
00:01:43,550 --> 00:01:46,625
can see we're going to get a term with expectation of X,

39
00:01:46,625 --> 00:01:48,350
one with expectation of X squared, one

40
00:01:48,350 --> 00:01:50,190
with expectation of X cubed.

41
00:01:50,190 --> 00:01:53,280
And each of those has a coefficient, t, t squared,

42
00:01:53,280 --> 00:01:54,590
t cubed, and so on.

43
00:01:54,590 --> 00:01:59,390
And by differentiating with respect to the t's, we

44
00:01:59,390 --> 00:02:03,140
can recover the individual coefficients

45
00:02:03,140 --> 00:02:05,700
or the individual moments of the distribution.

46
00:02:05,700 --> 00:02:08,660
So this is the same thing as a moment generating function,

47
00:02:08,660 --> 00:02:11,490
just with these extra powers of i tucked in.

48
00:02:11,490 --> 00:02:16,290
So this is especially useful if we can do it in closed form.

49
00:02:16,290 --> 00:02:19,520
So for example, in the case of the binomial distribution,

50
00:02:19,520 --> 00:02:26,240
where we can write down the probability distribution here

51
00:02:26,240 --> 00:02:32,870
for each value of k, then we can sum over k equals 0 to n.

52
00:02:32,870 --> 00:02:35,970
It gets cut off, we don't need to go all the way to infinity.

53
00:02:35,970 --> 00:02:38,510
And we can actually do this some in closed form.

54
00:02:38,510 --> 00:02:42,710
Because, notice, that the e to the itk is e to the it

55
00:02:42,710 --> 00:02:44,570
raised to the k-th power.

56
00:02:44,570 --> 00:02:49,970
I just regroup those together over here and do the sum.

57
00:02:49,970 --> 00:02:54,470
And that's where the binomial distribution gets its name.

58
00:02:54,470 --> 00:02:58,790
It's the expansion of this binomial, this quantity p

59
00:02:58,790 --> 00:03:01,760
e to the it plus q raised to the n-th power.

60
00:03:01,760 --> 00:03:02,540
So that's it.

61
00:03:02,540 --> 00:03:04,070
That's in closed form.

62
00:03:04,070 --> 00:03:06,500
That's the characteristic function

63
00:03:06,500 --> 00:03:08,550
of the binomial distribution.

64
00:03:08,550 --> 00:03:13,440
So if I want to get the moments from it, I take derivatives.

65
00:03:13,440 --> 00:03:14,680
So the first moment--

66
00:03:14,680 --> 00:03:18,860
so if I look up here up top, if I pick a value of l. l

67
00:03:18,860 --> 00:03:21,020
equals 0, 1, or 2.

68
00:03:21,020 --> 00:03:23,780
I come down here and I take zero derivatives or I

69
00:03:23,780 --> 00:03:26,720
take one derivative times a minus i.

70
00:03:26,720 --> 00:03:28,400
And that gives me np.

71
00:03:28,400 --> 00:03:31,830
I take two derivatives with a minus i squared,

72
00:03:31,830 --> 00:03:33,440
which gives me a minus 1.

73
00:03:33,440 --> 00:03:35,150
All you need to know is that i is

74
00:03:35,150 --> 00:03:36,890
the square root of negative 1.

75
00:03:36,890 --> 00:03:41,960
And here, I get npq plus n squared p squared, which is

76
00:03:41,960 --> 00:03:44,280
what we saw earlier, and so on.

77
00:03:44,280 --> 00:03:48,410
So this is just a nice way of encoding all

78
00:03:48,410 --> 00:03:52,340
of the moments in one place.

79
00:03:52,340 --> 00:03:54,680
And then we recover them by taking derivatives,

80
00:03:54,680 --> 00:03:57,050
if we can compute that in closed form.

81
00:03:57,050 --> 00:03:59,570
In the case of continuous distributions,

82
00:03:59,570 --> 00:04:03,590
the expectation of e to the itX takes the form of an integral.

83
00:04:03,590 --> 00:04:06,110
And this intergral over here on the left

84
00:04:06,110 --> 00:04:09,860
is known as a Fourier transform, up to some constants

85
00:04:09,860 --> 00:04:13,190
that we sometimes normalize with.

86
00:04:13,190 --> 00:04:15,472
This integral from minus infinity to infinity of e

87
00:04:15,472 --> 00:04:19,970
to the itx p of x dx is the Fourier transfer of p of x.

88
00:04:19,970 --> 00:04:23,420
And it's denoted by this function p tilde of t.

89
00:04:23,420 --> 00:04:26,060
And of course, doing the same rule as before,

90
00:04:26,060 --> 00:04:30,590
where we expand out this e to the itx in this Taylor series,

91
00:04:30,590 --> 00:04:34,200
interchanged the order of the summation and the integration,

92
00:04:34,200 --> 00:04:37,070
we see that this can be written as an infinite sum

93
00:04:37,070 --> 00:04:38,420
of expectations.

94
00:04:38,420 --> 00:04:40,460
And if we want to invert or if we

95
00:04:40,460 --> 00:04:46,280
want to identify a particular expectation,

96
00:04:46,280 --> 00:04:48,830
second the expectation of X squared,

97
00:04:48,830 --> 00:04:50,840
it's the coefficient of t squared.

98
00:04:50,840 --> 00:04:53,630
The way we get it is we take two derivatives

99
00:04:53,630 --> 00:04:55,430
and then set t equal 0.

100
00:04:55,430 --> 00:04:58,490
And then we've got these extra i's that are out in front.

101
00:04:58,490 --> 00:04:59,090
OK?

102
00:04:59,090 --> 00:05:02,810
So that's just an ordinary moment generating function

103
00:05:02,810 --> 00:05:04,880
for discrete and continuous examples.

104
00:05:04,880 --> 00:05:06,150
Why is it useful?

105
00:05:06,150 --> 00:05:08,870
Well, it's really useful for sums of random variables.

106
00:05:08,870 --> 00:05:11,040
And the reason is that when we have

107
00:05:11,040 --> 00:05:14,240
the sum of random variables, say, x plus y,

108
00:05:14,240 --> 00:05:18,040
that the full probability distribution is

109
00:05:18,040 --> 00:05:22,400
a convolution of the two probability distributions.

110
00:05:22,400 --> 00:05:27,010
So if x_1 and x_2 are arbitrary variables, the probability

111
00:05:27,010 --> 00:05:31,180
density function for y is going to be the integral of p

112
00:05:31,180 --> 00:05:34,960
of x_1 times p_2 of y minus x_1, where

113
00:05:34,960 --> 00:05:39,620
I integrate dx_1 to get a function of y.

114
00:05:39,620 --> 00:05:41,690
And that's a complicated interval to do.

115
00:05:41,690 --> 00:05:43,600
And then I do that for more and more sums.

116
00:05:43,600 --> 00:05:47,540
But these Fourier transforms have a marvelous property,

117
00:05:47,540 --> 00:05:50,410
which is that the Fourier transform of a convolution

118
00:05:50,410 --> 00:05:52,960
is the product of the Fourier transforms.

119
00:05:52,960 --> 00:06:01,910
So the Fourier transform for p of y

120
00:06:01,910 --> 00:06:05,375
is just p_1 tilde times p_2 tilde.

121
00:06:05,375 --> 00:06:07,280
We just multiply them together.

122
00:06:07,280 --> 00:06:09,670
And that makes it easy.

