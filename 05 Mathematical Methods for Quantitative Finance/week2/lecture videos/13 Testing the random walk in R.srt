0
00:00:00,000 --> 00:00:00,806


1
00:00:00,806 --> 00:00:03,610
PROFESSOR: In this video will test a random walk.

2
00:00:03,610 --> 00:00:05,650
We'll look at the computational and data

3
00:00:05,650 --> 00:00:08,560
tables that are needed to do an analysis,

4
00:00:08,560 --> 00:00:12,790
perform a variance ratio test, and explore whether asset

5
00:00:12,790 --> 00:00:15,400
prices may be predictable, whether the random walk

6
00:00:15,400 --> 00:00:19,090
hypothesis holds for a given data set or not.

7
00:00:19,090 --> 00:00:21,250
I'm doing this in an R notebook.

8
00:00:21,250 --> 00:00:25,060
And R notebooks are text files that contain code

9
00:00:25,060 --> 00:00:27,710
but they also contain discussion around them.

10
00:00:27,710 --> 00:00:30,250
So on the left hand side of my screen,

11
00:00:30,250 --> 00:00:32,950
you can see the raw contents of the text

12
00:00:32,950 --> 00:00:34,630
file of the R notebook.

13
00:00:34,630 --> 00:00:37,690
And you can load that into your own version of RStudio.

14
00:00:37,690 --> 00:00:41,980
And on the right hand side is the formatted version in HTML

15
00:00:41,980 --> 00:00:44,690
which you can do whatever you'd like with.

16
00:00:44,690 --> 00:00:47,800
And when you hit this preview button

17
00:00:47,800 --> 00:00:51,100
on the top of the screen, it'll regenerate

18
00:00:51,100 --> 00:00:53,325
the preview of the notebook.

19
00:00:53,325 --> 00:00:54,700
But right now what we'll be doing

20
00:00:54,700 --> 00:00:57,250
is mostly working on the left hand side.

21
00:00:57,250 --> 00:00:59,890
And we'll start by loading a few packages.

22
00:00:59,890 --> 00:01:04,120
Now packages in R are installed once

23
00:01:04,120 --> 00:01:06,040
and then they're run many times.

24
00:01:06,040 --> 00:01:09,580
So to install a package if you've never seen it,

25
00:01:09,580 --> 00:01:13,570
you uncomment these lines, it'll grab the package

26
00:01:13,570 --> 00:01:17,590
and run the package to install it on your computer.

27
00:01:17,590 --> 00:01:20,950
Subsequently, packages are loaded into a given session

28
00:01:20,950 --> 00:01:22,558
using the library command.

29
00:01:22,558 --> 00:01:24,100
So for right now I think the only one

30
00:01:24,100 --> 00:01:27,430
that we need here is going to be the tidyquant package that

31
00:01:27,430 --> 00:01:30,970
will help us with the function to fetch some data directly

32
00:01:30,970 --> 00:01:33,820
into R so that we don't need to load it from a flat file.

33
00:01:33,820 --> 00:01:36,710
That's a perfectly honorable thing to do.

34
00:01:36,710 --> 00:01:41,380
So I click here and run this current chunk,

35
00:01:41,380 --> 00:01:48,940
and it will run and load the packages of interest.

36
00:01:48,940 --> 00:01:53,770
We'll get a few descriptive messages that will go away

37
00:01:53,770 --> 00:01:54,860
the next time we run it.

38
00:01:54,860 --> 00:01:57,870
So those are not essential information

39
00:01:57,870 --> 00:01:59,100
or essential reading.

40
00:01:59,100 --> 00:02:00,930
Now let's take a look at some data.

41
00:02:00,930 --> 00:02:03,660
Specifically, we'll take a look at stock for Tootsie Roll

42
00:02:03,660 --> 00:02:09,699
when the longest continuously the companies that's

43
00:02:09,699 --> 00:02:13,060
been around publicly traded for a long period of time

44
00:02:13,060 --> 00:02:17,200
since the beginning of the end of the 19th century

45
00:02:17,200 --> 00:02:18,700
beginning of the 20th century.

46
00:02:18,700 --> 00:02:21,400
And we'll get some data for Tootsie Roll.

47
00:02:21,400 --> 00:02:23,590
And if we do grab data, there are a number

48
00:02:23,590 --> 00:02:27,280
of issues we normally need to be concerned about, which are,

49
00:02:27,280 --> 00:02:28,330
what's the data source?

50
00:02:28,330 --> 00:02:30,280
How do they handle corporate actions?

51
00:02:30,280 --> 00:02:32,410
What's the format of the file?

52
00:02:32,410 --> 00:02:33,910
How are the line breaks handled?

53
00:02:33,910 --> 00:02:35,900
What is delimiters and so on?

54
00:02:35,900 --> 00:02:37,750
So we could start with the CSV file

55
00:02:37,750 --> 00:02:43,640
or we could grab things directly from Yahoo Finance.

56
00:02:43,640 --> 00:02:46,140
So I'll show you the latter here.

57
00:02:46,140 --> 00:02:49,670
So what we'll do is we'll use this one function called

58
00:02:49,670 --> 00:02:53,300
tq_get() which is the tidyquant get function that will grab

59
00:02:53,300 --> 00:02:56,630
a given ticker symbol for a range of dates,

60
00:02:56,630 --> 00:02:58,520
and grab data from Yahoo Finance.

61
00:02:58,520 --> 00:02:59,420
So we'll do that now.

62
00:02:59,420 --> 00:03:02,000
The ticker symbol that will define here

63
00:03:02,000 --> 00:03:06,060
is tr for Tootsie Roll and we'll pick a range of dates.

64
00:03:06,060 --> 00:03:10,700
Now one of the things that we're going to do, so hit the Run

65
00:03:10,700 --> 00:03:13,670
button here and with fingers crossed because we're

66
00:03:13,670 --> 00:03:14,570
running this live.

67
00:03:14,570 --> 00:03:16,130
Will hope that everything looks.

68
00:03:16,130 --> 00:03:18,470
So one of the things that we commonly do

69
00:03:18,470 --> 00:03:22,370
is because we usually are more interested in returns

70
00:03:22,370 --> 00:03:26,960
than in prices, we need to get and we'll always

71
00:03:26,960 --> 00:03:30,450
need extra prices to be able to compute a return series.

72
00:03:30,450 --> 00:03:33,380
So we typically want to start retrieving data few days

73
00:03:33,380 --> 00:03:36,788
at least before the first return that we need to compute.

74
00:03:36,788 --> 00:03:38,330
So here I've done this for the period

75
00:03:38,330 --> 00:03:42,890
from 1988 to the end of 2017.

76
00:03:42,890 --> 00:03:46,370
So this is before the Wall Street Bets era.

77
00:03:46,370 --> 00:03:50,960
So to check that we've got the data, we can go to our console

78
00:03:50,960 --> 00:03:54,372
and take a look at a view data, or we can run and take a look

79
00:03:54,372 --> 00:03:55,205
at any of the lines.

80
00:03:55,205 --> 00:03:59,720


81
00:03:59,720 --> 00:04:03,980
So we can either say head TR to take a look

82
00:04:03,980 --> 00:04:10,250
at the first few rows, or we can say View with a capital V,

83
00:04:10,250 --> 00:04:12,030
to take a look at all of the data.

84
00:04:12,030 --> 00:04:15,500
And what we see is that, we've got here from Yahoo Finance

85
00:04:15,500 --> 00:04:18,019
is data structure where the first column is the ticker

86
00:04:18,019 --> 00:04:18,769
symbol.

87
00:04:18,769 --> 00:04:21,950
Then we have the date open high-low close volume.

88
00:04:21,950 --> 00:04:24,475
And the last column called adjusted is what we'll use.

89
00:04:24,475 --> 00:04:25,850
That's where the prices have been

90
00:04:25,850 --> 00:04:27,950
adjusted for corporate actions.

91
00:04:27,950 --> 00:04:31,460
Tootsie roll was an interesting and unusual dividend policy

92
00:04:31,460 --> 00:04:33,170
which we won't go into, we'll just

93
00:04:33,170 --> 00:04:37,450
be working with these adjusted prices.

94
00:04:37,450 --> 00:04:39,040
So, what does the data look like?

95
00:04:39,040 --> 00:04:41,830
Well, here's what the price chart looks like over time.

96
00:04:41,830 --> 00:04:44,040
So I'm going to click here, and I

97
00:04:44,040 --> 00:04:47,220
wanted to show you some code through example.

98
00:04:47,220 --> 00:04:49,240
I'm just using the base R plot function,

99
00:04:49,240 --> 00:04:52,980
we can make much nicer plots using the GG plot package.

100
00:04:52,980 --> 00:04:56,010
But here we're just doing vanilla plots,

101
00:04:56,010 --> 00:04:59,620
and we can dress them up later on.

102
00:04:59,620 --> 00:05:01,740
So here's the Tootsie Roll adjusted price

103
00:05:01,740 --> 00:05:02,890
over this period.

104
00:05:02,890 --> 00:05:04,690
So it's had a bumpy ride.

105
00:05:04,690 --> 00:05:06,930
But generally if you are invested in toastie roll,

106
00:05:06,930 --> 00:05:09,560
you've done pretty well.

107
00:05:09,560 --> 00:05:10,130
OK?

108
00:05:10,130 --> 00:05:13,430
Next, now notice that as I update this

109
00:05:13,430 --> 00:05:17,270
as we run chunks of code, that on the right hand

110
00:05:17,270 --> 00:05:22,110
side of my notebook up here, and as we scroll down,

111
00:05:22,110 --> 00:05:24,860
we'll get these graphs will be incorporated.

112
00:05:24,860 --> 00:05:28,130
And we can even include formulas by writing things, commands,

113
00:05:28,130 --> 00:05:28,970
and LaTeX.

114
00:05:28,970 --> 00:05:30,390
So we can get formulas here.

115
00:05:30,390 --> 00:05:31,310
So here's what we do.

116
00:05:31,310 --> 00:05:34,040
We defined the logarithmic returns,

117
00:05:34,040 --> 00:05:35,840
as being the logarithm of the price

118
00:05:35,840 --> 00:05:40,040
ratios, the last observed price divided by the previous price,

119
00:05:40,040 --> 00:05:43,580
and we see that can be written as a difference

120
00:05:43,580 --> 00:05:45,950
of successive price logarithms.

121
00:05:45,950 --> 00:05:49,850
So, in code what we're going to do is extract the prices,

122
00:05:49,850 --> 00:05:52,670
get the return series and most of our analysis

123
00:05:52,670 --> 00:05:56,510
really is going to be testing the returns themselves.

124
00:05:56,510 --> 00:05:58,910
Because the returns, when we talk about random walk we

125
00:05:58,910 --> 00:06:03,680
sometimes in finance because the context is usually clear,

126
00:06:03,680 --> 00:06:07,460
we may abuse the language a little bit.

127
00:06:07,460 --> 00:06:10,610
What we're talking about is the return series

128
00:06:10,610 --> 00:06:17,210
for a random walk being uncorrelated random variables

129
00:06:17,210 --> 00:06:18,020
over time.

130
00:06:18,020 --> 00:06:21,350
The price series does have levels that matter,

131
00:06:21,350 --> 00:06:23,930
and it's by taking these differences of logarithms

132
00:06:23,930 --> 00:06:26,000
and we get to the series of interest.

133
00:06:26,000 --> 00:06:27,500
So there are two things we could do.

134
00:06:27,500 --> 00:06:30,110
First, we could just strip off the data and work with that,

135
00:06:30,110 --> 00:06:34,182
or we can also store the data as part of our data structure.

136
00:06:34,182 --> 00:06:35,640
So there are two ways we can do it.

137
00:06:35,640 --> 00:06:37,140
First of all, what I can do is I can

138
00:06:37,140 --> 00:06:41,210
define a variable p that just pulls out the adjusted price

139
00:06:41,210 --> 00:06:42,050
process.

140
00:06:42,050 --> 00:06:45,050
I can say r is diff log p.

141
00:06:45,050 --> 00:06:48,950
That implements the formula that we

142
00:06:48,950 --> 00:06:52,820
see behind me you just move that up the screen a little bit.

143
00:06:52,820 --> 00:06:53,870
There we go.

144
00:06:53,870 --> 00:06:56,230
So for that formula right here.

145
00:06:56,230 --> 00:07:04,230


146
00:07:04,230 --> 00:07:07,170
Sorry, trying to highlight it, there we go.

147
00:07:07,170 --> 00:07:09,250
A little awkwardly, sorry about that.

148
00:07:09,250 --> 00:07:14,670
So anyway this formula here is the formula that we want.

149
00:07:14,670 --> 00:07:17,220
We get that in code by saying diff log R,

150
00:07:17,220 --> 00:07:19,080
you could write a for loop if you like just

151
00:07:19,080 --> 00:07:22,680
to take successive differences, and then we'll

152
00:07:22,680 --> 00:07:24,570
add and be the length of the series.

153
00:07:24,570 --> 00:07:26,940
Then what we can also do, let me just run this chunk

154
00:07:26,940 --> 00:07:27,750
as we're speaking.

155
00:07:27,750 --> 00:07:32,460
What we can also do is we can add a new column to our data

156
00:07:32,460 --> 00:07:33,130
structure.

157
00:07:33,130 --> 00:07:35,550
And when I do that, if I look at tr,

158
00:07:35,550 --> 00:07:41,160
we'll see that now there's a new column here R. At the end,

159
00:07:41,160 --> 00:07:45,220
I don't think I want things sorted by R, sorry about that.

160
00:07:45,220 --> 00:07:46,920
But here we've got this new column.

161
00:07:46,920 --> 00:07:49,980
So let's just see what we did in the code.

162
00:07:49,980 --> 00:07:56,100
What we did was we set a new column here for our TR.

163
00:07:56,100 --> 00:07:59,400
We added a column r, and we had to include a leading

164
00:07:59,400 --> 00:08:01,920
NA for the simple reason that we're always

165
00:08:01,920 --> 00:08:04,750
going to have one less return than we do prices.

166
00:08:04,750 --> 00:08:08,100
So if we want things to line up, the return corresponding

167
00:08:08,100 --> 00:08:09,987
to our very first price is not available

168
00:08:09,987 --> 00:08:11,820
because we don't know the price before that.

169
00:08:11,820 --> 00:08:14,790
But all the others will line up with their corresponding dates,

170
00:08:14,790 --> 00:08:17,760
and then that first date of interest we'll toss it out,

171
00:08:17,760 --> 00:08:20,520
so that everything is lined up starting on the first trading

172
00:08:20,520 --> 00:08:23,170
day of 1988 in this example.

173
00:08:23,170 --> 00:08:25,680
Now we can take the returns and then

174
00:08:25,680 --> 00:08:27,990
we can plot those, and see what those look like.

175
00:08:27,990 --> 00:08:30,000
And if we do that, we get a graph

176
00:08:30,000 --> 00:08:33,600
that you can see over here on the side,

177
00:08:33,600 --> 00:08:35,760
where we see the daily return series.

178
00:08:35,760 --> 00:08:39,179
Now, the daily return series is noisy.

179
00:08:39,179 --> 00:08:42,870
And the mean return is not really visible on this scale.

180
00:08:42,870 --> 00:08:46,530
And this is typical of a lot of financial return data,

181
00:08:46,530 --> 00:08:49,680
that on very short time scales like a day or less,

182
00:08:49,680 --> 00:08:53,370
it's almost impossible to see the mean return.

183
00:08:53,370 --> 00:08:55,170
But you certainly can see the risk.

184
00:08:55,170 --> 00:08:59,470
You certainly can see the fluctuations in value.

185
00:08:59,470 --> 00:09:01,260
So we'll use some statistics to get

186
00:09:01,260 --> 00:09:04,320
at the heart of what's going on to both characterize,

187
00:09:04,320 --> 00:09:06,240
to describe the data, and then look

188
00:09:06,240 --> 00:09:09,415
at what we can say about the relationship across time

189
00:09:09,415 --> 00:09:11,040
and about any predictability that might

190
00:09:11,040 --> 00:09:13,720
be present in the data series.

191
00:09:13,720 --> 00:09:16,680
So let's just compare this picture

192
00:09:16,680 --> 00:09:20,790
with what it would look like if it were a pure white noise

193
00:09:20,790 --> 00:09:21,640
series.

194
00:09:21,640 --> 00:09:23,640
So what I'm going to do here, is I'm

195
00:09:23,640 --> 00:09:28,320
going to generate using the rnorm function.

196
00:09:28,320 --> 00:09:34,230
A set of random variables drawn from a normal distribution

197
00:09:34,230 --> 00:09:39,480
that are scaled to have 0 mean and the same standard deviation

198
00:09:39,480 --> 00:09:41,400
as the Tootsie Roll data does.

199
00:09:41,400 --> 00:09:46,230
So we can see at a glance this is hardly a statistical test,

200
00:09:46,230 --> 00:09:49,560
but we can see that they don't exactly look the same.

201
00:09:49,560 --> 00:09:51,900
That is the white noise process is noisy,

202
00:09:51,900 --> 00:09:54,330
but it's much more uniform over time.

203
00:09:54,330 --> 00:09:58,230
Whereas the Tootsie Roll process has not only some pretty big

204
00:09:58,230 --> 00:10:00,210
spikes that come along, but there

205
00:10:00,210 --> 00:10:02,730
are periods of intense fluctuation

206
00:10:02,730 --> 00:10:04,710
and some periods of quiet.

207
00:10:04,710 --> 00:10:07,650
So what this might suggest is that volatility

208
00:10:07,650 --> 00:10:10,450
itself can vary over time.

209
00:10:10,450 --> 00:10:13,590
And then when we talk about the volatility of a process,

210
00:10:13,590 --> 00:10:16,710
maybe that's something that we would eventually

211
00:10:16,710 --> 00:10:18,770
need to generalize.

212
00:10:18,770 --> 00:10:19,640
OK?

213
00:10:19,640 --> 00:10:22,640
So that's just in pictures for looking at our data.

214
00:10:22,640 --> 00:10:25,650
We can get some summary statistics for the data.

215
00:10:25,650 --> 00:10:27,980
So we can use the summary command in R

216
00:10:27,980 --> 00:10:30,470
to give us a look at our data set.

217
00:10:30,470 --> 00:10:33,920
And it tells us for each of our data fields

218
00:10:33,920 --> 00:10:36,343
open, high-low, close, and the return.

219
00:10:36,343 --> 00:10:37,010
What's the mean?

220
00:10:37,010 --> 00:10:37,730
What's the max?

221
00:10:37,730 --> 00:10:38,300
What's mean?

222
00:10:38,300 --> 00:10:41,490
What's the median, interquartile range, and so on.

223
00:10:41,490 --> 00:10:44,930
So we've got 7,500 observations which isn't bad,

224
00:10:44,930 --> 00:10:47,360
and we can see a range of the numbers.

225
00:10:47,360 --> 00:10:50,360
Now these numbers in particular, if we look at the returns

226
00:10:50,360 --> 00:10:52,560
are pretty small because their daily numbers.

227
00:10:52,560 --> 00:10:56,690
So the convention is that we annualize numbers

228
00:10:56,690 --> 00:10:59,240
when we're looking at return and risk data.

229
00:10:59,240 --> 00:11:04,790
And that means that for our annualization conventions,

230
00:11:04,790 --> 00:11:07,190
that when it comes to returns, we're

231
00:11:07,190 --> 00:11:10,050
going to multiply by the number of time periods.

232
00:11:10,050 --> 00:11:12,520
And when it comes two standard deviations,

233
00:11:12,520 --> 00:11:14,270
we're going to multiply by the square root

234
00:11:14,270 --> 00:11:15,720
of the number of time periods.

235
00:11:15,720 --> 00:11:18,710
And this is just a convention for scaling

236
00:11:18,710 --> 00:11:20,630
our descriptive statistics.

237
00:11:20,630 --> 00:11:24,740
It would be natural if, in fact, we

238
00:11:24,740 --> 00:11:28,010
have random walks, because we know that the variance scales

239
00:11:28,010 --> 00:11:30,290
with the square that with time linearly,

240
00:11:30,290 --> 00:11:32,600
and the standard deviation with its square root.

241
00:11:32,600 --> 00:11:34,670
We don't know how this data behaves,

242
00:11:34,670 --> 00:11:37,950
but by convention when we report descriptive statistics,

243
00:11:37,950 --> 00:11:39,860
we typically use these conventions.

244
00:11:39,860 --> 00:11:41,540
Why 252?

245
00:11:41,540 --> 00:11:44,850
It's the typical number of trading days in the US equity

246
00:11:44,850 --> 00:11:45,350
markets.

247
00:11:45,350 --> 00:11:47,920


248
00:11:47,920 --> 00:11:51,900
Now, if we look at what that would correspond to, let's

249
00:11:51,900 --> 00:11:54,570
run this, and let's take a look at how

250
00:11:54,570 --> 00:11:56,450
we would get the numbers.

251
00:11:56,450 --> 00:12:00,330
So if we take the mean return and multiply by 252,

252
00:12:00,330 --> 00:12:02,220
and we take the standard deviation

253
00:12:02,220 --> 00:12:05,010
and we multiplied by the square root of 252,

254
00:12:05,010 --> 00:12:10,530
we find that we get a return, annualized return of about 8.3%

255
00:12:10,530 --> 00:12:14,010
with a volatility of about 24%, which

256
00:12:14,010 --> 00:12:19,950
is not unusual for US stock.

257
00:12:19,950 --> 00:12:22,710
Now let's take a look at the histogram of the returns,

258
00:12:22,710 --> 00:12:25,200
and see what their distribution looks like.

259
00:12:25,200 --> 00:12:28,770
Now the histogram is very sharply peaked in the middle.

260
00:12:28,770 --> 00:12:32,400
So it doesn't look Gaussian, it's not rounded over the top.

261
00:12:32,400 --> 00:12:36,600
It's kind of squished in the middle, which is interesting.

262
00:12:36,600 --> 00:12:42,420
But it is single peaked and if you remember,

263
00:12:42,420 --> 00:12:48,270
our discussion of the attributes of a random walk, and of time

264
00:12:48,270 --> 00:12:52,950
series models generally use primarily the moment

265
00:12:52,950 --> 00:12:55,420
of the distribution not the full distribution.

266
00:12:55,420 --> 00:12:59,460
So when we're interested in the sum of the random variables,

267
00:12:59,460 --> 00:13:01,950
whether the individual random variables are

268
00:13:01,950 --> 00:13:04,920
drawn from a Gaussian or a log normal distribution,

269
00:13:04,920 --> 00:13:07,650
or whether there binomial coin flips

270
00:13:07,650 --> 00:13:11,130
doesn't make a difference in terms of the large time scale

271
00:13:11,130 --> 00:13:13,260
properties when we add together a large number

272
00:13:13,260 --> 00:13:17,040
of random variables, and we ask about the mean and variance

273
00:13:17,040 --> 00:13:17,910
of the sum.

274
00:13:17,910 --> 00:13:22,230
But it's always a good idea to look at your data.

275
00:13:22,230 --> 00:13:26,780
So Lo and MacKinlay asked about whether the variance

276
00:13:26,780 --> 00:13:30,740
of a sample of returns, in fact, grows linearly

277
00:13:30,740 --> 00:13:33,120
as a function of the observation interval?

278
00:13:33,120 --> 00:13:35,510
So the difference is, what they did

279
00:13:35,510 --> 00:13:38,300
is instead of letting the time and just adding

280
00:13:38,300 --> 00:13:41,450
more and more days where the market conditions might

281
00:13:41,450 --> 00:13:44,660
have changed, they ask about taking a fixed number of days

282
00:13:44,660 --> 00:13:47,210
and looking at different course screening.

283
00:13:47,210 --> 00:13:49,820
So we can look at daily observations,

284
00:13:49,820 --> 00:13:51,470
then we can go to two day periods.

285
00:13:51,470 --> 00:13:52,760
We can go to four day periods.

286
00:13:52,760 --> 00:13:55,340
We can go to one week, one month, and so on.

287
00:13:55,340 --> 00:13:56,930
So they did powers of 2.

288
00:13:56,930 --> 00:13:59,510
Here we'll just run a for loop and do different numbers

289
00:13:59,510 --> 00:14:03,800
of days from two days through 100 days,

290
00:14:03,800 --> 00:14:04,995
and let's take a look at it.

291
00:14:04,995 --> 00:14:06,620
What we're doing here is, we're looking

292
00:14:06,620 --> 00:14:10,370
at the variance, of the difference, of the logarithms,

293
00:14:10,370 --> 00:14:11,420
of the price.

294
00:14:11,420 --> 00:14:13,280
And the price what we're doing in our

295
00:14:13,280 --> 00:14:17,090
we're looking at taking steps but of size n.

296
00:14:17,090 --> 00:14:19,100
Now that's going to leave in the event

297
00:14:19,100 --> 00:14:26,810
where n does not evenly divide 7,561 or 7562 observations

298
00:14:26,810 --> 00:14:27,530
that we have.

299
00:14:27,530 --> 00:14:30,120
That will leave a few points left over at the end.

300
00:14:30,120 --> 00:14:32,250
So this is just to give us a rough idea.

301
00:14:32,250 --> 00:14:35,840
It's not as precise as making sure that we divide things,

302
00:14:35,840 --> 00:14:39,500
but we can see at a glance that we've got a generally

303
00:14:39,500 --> 00:14:40,970
linear behavior.

304
00:14:40,970 --> 00:14:43,280
And now we could ask, so does that settle the matter.

305
00:14:43,280 --> 00:14:45,200
Does that say a yes, the variance

306
00:14:45,200 --> 00:14:47,030
grows linearly with time.

307
00:14:47,030 --> 00:14:48,740
Well, we don't know what the slope

308
00:14:48,740 --> 00:14:51,380
is, we don't know it's exactly linear,

309
00:14:51,380 --> 00:14:53,780
and after all, there is some scatter around

310
00:14:53,780 --> 00:14:57,295
and the scatter gets worse as we go further out.

311
00:14:57,295 --> 00:14:58,670
Now that could be because we have

312
00:14:58,670 --> 00:15:02,465
a breakdown of the hypothesis as we get to larger values.

313
00:15:02,465 --> 00:15:04,700
And then, on the other hand, it could just

314
00:15:04,700 --> 00:15:07,460
be that as our observation windows get larger,

315
00:15:07,460 --> 00:15:08,990
there are fewer of them and we're

316
00:15:08,990 --> 00:15:12,680
going to have less statistical significance

317
00:15:12,680 --> 00:15:15,140
in our observations, and we would think that things

318
00:15:15,140 --> 00:15:18,740
would be somewhat noisier.

319
00:15:18,740 --> 00:15:21,380
So let's take a look at some of these metrics.

320
00:15:21,380 --> 00:15:23,130
Now this is the code.

321
00:15:23,130 --> 00:15:25,710
And let me move this over here.

322
00:15:25,710 --> 00:15:27,120
So you can see the code.

323
00:15:27,120 --> 00:15:30,650
So this is the code that's in the Lo and MacKinlay paper,

324
00:15:30,650 --> 00:15:36,440
and that I discussed in lecture where we can construct

325
00:15:36,440 --> 00:15:39,920
a variety of different flavors of variants,

326
00:15:39,920 --> 00:15:42,020
where we might have overlapping windows,

327
00:15:42,020 --> 00:15:46,458
we have some scale adjustments, some removal of bias factors.

328
00:15:46,458 --> 00:15:48,500
But the basic idea is that what we're going to do

329
00:15:48,500 --> 00:15:52,040
is construct an adjusted variance for our data,

330
00:15:52,040 --> 00:15:54,350
get all the scaling and all the pre factors

331
00:15:54,350 --> 00:15:56,330
right on our Lo and MacKinlay.

332
00:15:56,330 --> 00:16:00,710
And then, we're going to compute the sampling statistic

333
00:16:00,710 --> 00:16:02,060
for the variance ratio.

334
00:16:02,060 --> 00:16:04,500
So remember that in the ideal case,

335
00:16:04,500 --> 00:16:07,430
the variance ratio where we take the variance

336
00:16:07,430 --> 00:16:12,060
divide by n or q times the, if q is the number of days

337
00:16:12,060 --> 00:16:16,190
that we're aggregating, that if we take the variance

338
00:16:16,190 --> 00:16:18,590
and take the ratio to the day variance,

339
00:16:18,590 --> 00:16:20,570
and we include a factor of q to scale out

340
00:16:20,570 --> 00:16:24,380
the obvious overall dependence, that variance ratio should

341
00:16:24,380 --> 00:16:29,330
be equal to 1 if there's a random walk.

342
00:16:29,330 --> 00:16:31,675
But the question is not do we get.

343
00:16:31,675 --> 00:16:33,050
One, we're going to get something

344
00:16:33,050 --> 00:16:36,440
in the neighborhood of one this would be what we'd expect.

345
00:16:36,440 --> 00:16:39,770
But how significant are the deviations from one.

346
00:16:39,770 --> 00:16:42,980
And to do that, we have the Z statistic.

347
00:16:42,980 --> 00:16:45,390
And the Z statistic is constructed

348
00:16:45,390 --> 00:16:48,980
so that it is sampled from an n zero 1 distribution.

349
00:16:48,980 --> 00:16:53,330
That is it should be Gaussian distributed with 0 mean

350
00:16:53,330 --> 00:16:55,800
and unit variance itself.

351
00:16:55,800 --> 00:17:00,590
So a Z that falls in the range of between plus or minus 2

352
00:17:00,590 --> 00:17:04,579
or maybe even plus or minus 3, could

353
00:17:04,579 --> 00:17:07,310
be consistent with a hypothesis being true.

354
00:17:07,310 --> 00:17:11,720
Large values of the Z, outside of plus or minus 2

355
00:17:11,720 --> 00:17:14,210
would be considered a significant departure.

356
00:17:14,210 --> 00:17:16,160
And if we were to do different tests

357
00:17:16,160 --> 00:17:22,560
we'd expect the Z's to be uncorrelated with each other.

358
00:17:22,560 --> 00:17:23,450
So let's take a look.

359
00:17:23,450 --> 00:17:25,760
So we can run this now that we've got our data,

360
00:17:25,760 --> 00:17:27,440
and we have our return series.

361
00:17:27,440 --> 00:17:32,330
We define this function variance.c for a function

362
00:17:32,330 --> 00:17:35,090
and we'll run it on our data set.

363
00:17:35,090 --> 00:17:37,850
OK, so let's run this chunk.

364
00:17:37,850 --> 00:17:40,760
And let's take a look at some of the results that we get.

365
00:17:40,760 --> 00:17:43,850
Now again if I want to update things on the right hand side,

366
00:17:43,850 --> 00:17:49,740
I click the Preview button just above my window.

367
00:17:49,740 --> 00:17:56,340
And now things will be update on the right hand side as well.

368
00:17:56,340 --> 00:18:00,220
And here's a bar plot of the results of our variance ratio

369
00:18:00,220 --> 00:18:01,070
test.

370
00:18:01,070 --> 00:18:03,970
So what we can see is that all of the values

371
00:18:03,970 --> 00:18:04,972
have the same sign.

372
00:18:04,972 --> 00:18:05,930
These are all negative.

373
00:18:05,930 --> 00:18:08,830
Of course, each of these windows these are not independent

374
00:18:08,830 --> 00:18:10,690
data samples were doing different levels

375
00:18:10,690 --> 00:18:14,230
of aggregation, but what we see is that a lot of the values

376
00:18:14,230 --> 00:18:16,690
are greatly in excess of minus 2.

377
00:18:16,690 --> 00:18:18,640
In fact, almost all of these are.

378
00:18:18,640 --> 00:18:22,220
So that means that in our Tootsie Roll data

379
00:18:22,220 --> 00:18:26,650
set that we can reject the random walk

380
00:18:26,650 --> 00:18:29,170
hypothesis for this range of values,

381
00:18:29,170 --> 00:18:32,930
and for this time period within the data set.

382
00:18:32,930 --> 00:18:33,740
OK?

383
00:18:33,740 --> 00:18:37,850
So what you can do is certainly you can go back and take

384
00:18:37,850 --> 00:18:40,460
a look at rerunning this or you should run it first yourself,

385
00:18:40,460 --> 00:18:42,710
but you can go back and change pretty much everything.

386
00:18:42,710 --> 00:18:46,430
You can pick a different stock, and you can pick the same stock

387
00:18:46,430 --> 00:18:50,060
in a different range, or you can change all of this,

388
00:18:50,060 --> 00:18:52,377
rerun and take a look and see what you get.

389
00:18:52,377 --> 00:18:53,960
I'd encourage you to take a look at it

390
00:18:53,960 --> 00:18:57,230
for a couple of stocks of interest to you,

391
00:18:57,230 --> 00:18:59,420
and also to take a look at stock indices.

392
00:18:59,420 --> 00:19:01,580
We might expect stocks and stock indices

393
00:19:01,580 --> 00:19:06,440
to behave differently after all stock index has values that

394
00:19:06,440 --> 00:19:08,750
are generated by a weighted sum of a bunch

395
00:19:08,750 --> 00:19:10,820
of different companies, which may have

396
00:19:10,820 --> 00:19:14,730
low correlation to each other.

397
00:19:14,730 --> 00:19:17,370
So some of the noise may average out.

398
00:19:17,370 --> 00:19:21,230
So I would encourage you to take this code, or even better

399
00:19:21,230 --> 00:19:23,360
to write your own, and to run the tests

400
00:19:23,360 --> 00:19:25,590
on some data of your own.

401
00:19:25,590 --> 00:19:27,290
So just one final thing to take a look

402
00:19:27,290 --> 00:19:32,000
at a, what this means in terms of volatility

403
00:19:32,000 --> 00:19:38,000
is, if we were to go back and construct the returns

404
00:19:38,000 --> 00:19:38,820
and scale them.

405
00:19:38,820 --> 00:19:41,450
So if we take the variance and we now multiply times

406
00:19:41,450 --> 00:19:45,110
square root of q or square root of n

407
00:19:45,110 --> 00:19:49,380
to the appropriate numerator denominator.

408
00:19:49,380 --> 00:19:51,110
So that we get things normalized.

409
00:19:51,110 --> 00:19:52,970
If we take a look here what I've defined

410
00:19:52,970 --> 00:19:56,690
is sigma n to be scaled with 1 over square root of n

411
00:19:56,690 --> 00:19:58,520
times the standard deviation.

412
00:19:58,520 --> 00:20:01,520
And instead of looking at is the plot linear,

413
00:20:01,520 --> 00:20:04,520
let's take out that expected dependence

414
00:20:04,520 --> 00:20:06,500
and ask if our results are constant.

415
00:20:06,500 --> 00:20:09,440
After all, that's what we're looking at in our Z statistic.

416
00:20:09,440 --> 00:20:12,980
The idea of something that is a random walk

417
00:20:12,980 --> 00:20:16,610
by the variance ratio test is, that no matter what aggregation

418
00:20:16,610 --> 00:20:18,410
scale we look at, we should always

419
00:20:18,410 --> 00:20:20,520
get the same value for the variance.

420
00:20:20,520 --> 00:20:22,760
So if we do that, we find that we

421
00:20:22,760 --> 00:20:25,010
get kind of a bumpy ride for different values,

422
00:20:25,010 --> 00:20:27,740
but we wouldn't expect it to be exactly flat.

423
00:20:27,740 --> 00:20:31,340
And the values are all in the 20 to 30%

424
00:20:31,340 --> 00:20:35,630
range, which are reasonable for a stock.

425
00:20:35,630 --> 00:20:40,310
If we want to get a rough idea without the precise statistical

426
00:20:40,310 --> 00:20:42,350
analysis, then what we could do is

427
00:20:42,350 --> 00:20:46,040
we might want to compare that again to simulated data,

428
00:20:46,040 --> 00:20:47,810
and run a Monte Carlo to generate

429
00:20:47,810 --> 00:20:51,950
data that could have come from a white noise random walk

430
00:20:51,950 --> 00:20:53,780
process.

431
00:20:53,780 --> 00:20:55,400
And we could ask what we would expect

432
00:20:55,400 --> 00:20:58,890
to see for its variance just due to sampling error.

433
00:20:58,890 --> 00:21:02,450
So this is just a visualization to help give us an intuitive

434
00:21:02,450 --> 00:21:05,600
sense of the statistical test on the one hand,

435
00:21:05,600 --> 00:21:08,450
and we have an intuitive pictorial sense,

436
00:21:08,450 --> 00:21:11,130
and I think both that reasoning might be helpful.

437
00:21:11,130 --> 00:21:13,310
So in this case you've got some code

438
00:21:13,310 --> 00:21:15,500
that implements a variance ratio test,

439
00:21:15,500 --> 00:21:18,110
you've got some code that will go grab data from Yahoo

440
00:21:18,110 --> 00:21:21,320
Finance, or if you can put it in yourself manually from any data

441
00:21:21,320 --> 00:21:22,770
source that you like.

442
00:21:22,770 --> 00:21:24,440
You can run the test and take a look

443
00:21:24,440 --> 00:21:27,830
at the behavior of the variance as you

444
00:21:27,830 --> 00:21:29,810
run through the different aggregations.

445
00:21:29,810 --> 00:21:33,350
And you can take a look at the detailed statistical analysis

446
00:21:33,350 --> 00:21:37,790
for Lo and MacKinlay carefully normalized Z statistic,

447
00:21:37,790 --> 00:21:41,270
and see for your data set whether you can reject

448
00:21:41,270 --> 00:21:44,140
the random walk hypothesis.

