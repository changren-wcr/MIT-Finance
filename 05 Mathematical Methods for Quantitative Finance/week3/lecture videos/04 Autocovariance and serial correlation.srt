0
00:00:00,000 --> 00:00:00,708


1
00:00:00,708 --> 00:00:02,250
PROFESSOR: Let's look at alternatives

2
00:00:02,250 --> 00:00:03,840
to the random walk.

3
00:00:03,840 --> 00:00:07,500
If we analyze market data for example for stock prices

4
00:00:07,500 --> 00:00:11,040
in the style of Lo and McKinley doing a variance ratio test,

5
00:00:11,040 --> 00:00:14,610
one of the ways that we can see that the random walk model is

6
00:00:14,610 --> 00:00:17,190
not a good fit for the data is the presence

7
00:00:17,190 --> 00:00:19,530
of serial correlation in returns.

8
00:00:19,530 --> 00:00:23,430
That is, we see that the returns are not in fact independent.

9
00:00:23,430 --> 00:00:25,830
If they were independent, the scaling behavior

10
00:00:25,830 --> 00:00:29,340
as we add together more and more terms would grow.

11
00:00:29,340 --> 00:00:31,780
The variance would grow linearly with time.

12
00:00:31,780 --> 00:00:33,840
And if we observed departures from that,

13
00:00:33,840 --> 00:00:36,680
that doesn't just mean that we reject the random walk.

14
00:00:36,680 --> 00:00:38,760
It gives us a clue as to what we might

15
00:00:38,760 --> 00:00:40,870
want to use to replace it.

16
00:00:40,870 --> 00:00:43,680
So if we're constructing aggregating returns,

17
00:00:43,680 --> 00:00:47,740
say q terms at a time, if they're not independent,

18
00:00:47,740 --> 00:00:50,970
then when we compute the variance the cross terms don't

19
00:00:50,970 --> 00:00:53,160
vanish when we take expectations.

20
00:00:53,160 --> 00:00:55,260
And that's telling us something important.

21
00:00:55,260 --> 00:00:57,660
That's telling us that information is propagating

22
00:00:57,660 --> 00:00:59,530
from one step to the next.

23
00:00:59,530 --> 00:01:02,220
So we want to think about autocovariance

24
00:01:02,220 --> 00:01:05,400
and autocorrelations, which measure the relationship

25
00:01:05,400 --> 00:01:07,270
of the time series with itself.

26
00:01:07,270 --> 00:01:09,400
That's what the auto means.

27
00:01:09,400 --> 00:01:13,060
And we just shift in time by k time steps.

28
00:01:13,060 --> 00:01:14,980
So we want to do this in two ways.

29
00:01:14,980 --> 00:01:17,970
One of them is an expectation for looking

30
00:01:17,970 --> 00:01:21,893
at unrealized, unobserved variables in the future.

31
00:01:21,893 --> 00:01:23,310
And the other is we'd like to look

32
00:01:23,310 --> 00:01:26,760
at estimators for these properties based on past data.

33
00:01:26,760 --> 00:01:30,370
And, of course, we'd expect these to be related.

34
00:01:30,370 --> 00:01:33,720
So the definitions that we have are going to be--

35
00:01:33,720 --> 00:01:38,760
and our notation will be gamma k for the covariance between two

36
00:01:38,760 --> 00:01:40,230
r's of different lags.

37
00:01:40,230 --> 00:01:42,510
When we have a stationary process,

38
00:01:42,510 --> 00:01:45,990
the key thing is it should only depend on the difference

39
00:01:45,990 --> 00:01:47,680
between the time indices.

40
00:01:47,680 --> 00:01:50,190
So t minus t minus k is k.

41
00:01:50,190 --> 00:01:52,080
And that's why I put the label over here.

42
00:01:52,080 --> 00:01:54,600
Notice the gamma k doesn't depend on t.

43
00:01:54,600 --> 00:01:56,880
It only depends on the time difference

44
00:01:56,880 --> 00:02:00,270
on the number of steps between the two arguments,

45
00:02:00,270 --> 00:02:03,120
in this case rt and rt minus k.

46
00:02:03,120 --> 00:02:05,610
Gamma 0 is when I said k equals 0.

47
00:02:05,610 --> 00:02:08,280
And that's the covariance of rt with itself.

48
00:02:08,280 --> 00:02:11,310
And rho sub k is just a normalized version

49
00:02:11,310 --> 00:02:12,600
divided by the variance.

50
00:02:12,600 --> 00:02:17,640
In the same way that we usually compute the correlation of two

51
00:02:17,640 --> 00:02:24,420
random variables by dividing by the square root of the product

52
00:02:24,420 --> 00:02:27,150
of the variances, if it's the same variable,

53
00:02:27,150 --> 00:02:30,210
if it's an autocovariance and we want

54
00:02:30,210 --> 00:02:32,700
to get the autocorrelation, we divide

55
00:02:32,700 --> 00:02:36,000
by the square root of the variance twice, variance times

56
00:02:36,000 --> 00:02:37,890
variance, of the same series.

57
00:02:37,890 --> 00:02:39,670
The series is stationary.

58
00:02:39,670 --> 00:02:43,110
That's equivalent to just gamma k divided by gamma 0.

59
00:02:43,110 --> 00:02:44,520
And this will be normalized.

60
00:02:44,520 --> 00:02:46,890
This will be a pure number, whereas gamma

61
00:02:46,890 --> 00:02:50,080
has units of r squared if r has dimensions.

62
00:02:50,080 --> 00:02:53,680
So for instance, if we think about a sum of two

63
00:02:53,680 --> 00:02:58,170
one-day returns, the variance, if they were uncorrelated,

64
00:02:58,170 --> 00:03:00,780
would be twice the variance of the one day returns.

65
00:03:00,780 --> 00:03:04,120
But if they are correlated there's an extra term.

66
00:03:04,120 --> 00:03:10,570
So the variance of a two-day return, which is--

67
00:03:10,570 --> 00:03:13,910
see if I can get my pointer back--

68
00:03:13,910 --> 00:03:17,030
well, if not we'll use our pen.

69
00:03:17,030 --> 00:03:18,990
The variance of a two-day return,

70
00:03:18,990 --> 00:03:22,490
which is just the variance of two consecutive days,

71
00:03:22,490 --> 00:03:25,700
is going to be twice the variance.

72
00:03:25,700 --> 00:03:27,350
It's going to be the variance of rt

73
00:03:27,350 --> 00:03:30,470
plus the variance of rt minus 1-- it's stationary,

74
00:03:30,470 --> 00:03:32,570
so those are identical-- plus twice

75
00:03:32,570 --> 00:03:34,670
the cross term in expectation.

76
00:03:34,670 --> 00:03:37,760
And that's going to be the covariance r1 and r2.

77
00:03:37,760 --> 00:03:41,990
So we can write that is twice the variance times 1

78
00:03:41,990 --> 00:03:48,200
plus rho 1, where row 1 is the lag 1 autocorrelation function.

79
00:03:48,200 --> 00:03:51,590
That is, it's the correlation of rt with rt minus 1.

80
00:03:51,590 --> 00:03:54,710


81
00:03:54,710 --> 00:03:58,940
So this is going to be bigger or smaller than twice the variance

82
00:03:58,940 --> 00:04:01,340
of a single period return depending

83
00:04:01,340 --> 00:04:05,250
on whether the autocorrelation is positive or negative,

84
00:04:05,250 --> 00:04:07,790
depending on whether the return in one period

85
00:04:07,790 --> 00:04:12,320
is more likely to have the same sign or the opposite sign

86
00:04:12,320 --> 00:04:15,090
of the return in the previous period.

87
00:04:15,090 --> 00:04:19,070
So in order to estimate and compute,

88
00:04:19,070 --> 00:04:25,550
we use our usual formulas for computing covariances

89
00:04:25,550 --> 00:04:28,340
of statistical data and for computing correlations

90
00:04:28,340 --> 00:04:31,140
and statistical data with one caveat.

91
00:04:31,140 --> 00:04:34,310
Normally, we take two time series of equal length.

92
00:04:34,310 --> 00:04:37,820
And to compute their covariance, we subtract off

93
00:04:37,820 --> 00:04:38,960
the mean of each one.

94
00:04:38,960 --> 00:04:41,640
We compute their products and each time period,

95
00:04:41,640 --> 00:04:43,430
and we sum over the time periods.

96
00:04:43,430 --> 00:04:45,010
We have a little bit of an issue when

97
00:04:45,010 --> 00:04:47,660
we look at finite length observations

98
00:04:47,660 --> 00:04:49,820
from actual historical data.

99
00:04:49,820 --> 00:04:53,120
And that suppose we have a series of 100 days.

100
00:04:53,120 --> 00:04:57,000
If we shift by 1, they'll only have 99% days in common.

101
00:04:57,000 --> 00:04:59,420
That will be one off the front and one off the back.

102
00:04:59,420 --> 00:05:00,530
So we need to adjust.

103
00:05:00,530 --> 00:05:04,430
And as we increase the lag s for a given dataset,

104
00:05:04,430 --> 00:05:07,590
we're going to have shorter and shorter lengths in common.

105
00:05:07,590 --> 00:05:10,460
So we need to be careful of that that we don't run out of data

106
00:05:10,460 --> 00:05:12,780
and that we look at the common period.

107
00:05:12,780 --> 00:05:15,530
Usually we make sure that we have a series that

108
00:05:15,530 --> 00:05:17,870
are long enough that the end points won't matter,

109
00:05:17,870 --> 00:05:19,490
that they'll just be small deviations.

110
00:05:19,490 --> 00:05:20,930
But we do need to check that.

111
00:05:20,930 --> 00:05:24,800
Alternatively, we can require that we

112
00:05:24,800 --> 00:05:26,712
have data of a certain minimum length

113
00:05:26,712 --> 00:05:28,670
by saying that we need to have a certain number

114
00:05:28,670 --> 00:05:29,720
of common points.

115
00:05:29,720 --> 00:05:32,670
And then we can account for the overhang for the lag.

116
00:05:32,670 --> 00:05:38,330
So if we want to go up to lag 20, which we would rarely need,

117
00:05:38,330 --> 00:05:41,080
we'd want to make sure that we have enough for 20 points

118
00:05:41,080 --> 00:05:43,580
overhanging at the beginning and enough points in the middle

119
00:05:43,580 --> 00:05:45,600
after we chop off those points.

120
00:05:45,600 --> 00:05:49,970
So the formula we have really is just a--

121
00:05:49,970 --> 00:05:53,360
very much like the formula for computing the covariance of two

122
00:05:53,360 --> 00:05:54,710
random variables.

123
00:05:54,710 --> 00:06:00,320
Subtract the mean, subtract the estimate of the mean.

124
00:06:00,320 --> 00:06:02,870
These should be the same, but we usually

125
00:06:02,870 --> 00:06:05,720
want to compute them appropriately for the length

126
00:06:05,720 --> 00:06:07,220
of the series it enters.

127
00:06:07,220 --> 00:06:10,190
So that because we're not using exactly the same data

128
00:06:10,190 --> 00:06:12,470
series because of the offset because some points are

129
00:06:12,470 --> 00:06:15,870
missing, we want to use the mean for each series.

130
00:06:15,870 --> 00:06:18,470
So it's removed-- we subtract off

131
00:06:18,470 --> 00:06:24,530
the sample mean for the dataset that's there and we compute.

132
00:06:24,530 --> 00:06:27,230
Now, we expect in general to see convergence

133
00:06:27,230 --> 00:06:28,710
depending on the details.

134
00:06:28,710 --> 00:06:31,580
So as t goes to infinity, or as we take a very large number

135
00:06:31,580 --> 00:06:37,250
of points, this estimate should converge to the true gamma k.

136
00:06:37,250 --> 00:06:40,400
And the sampling distributions we can see convergence

137
00:06:40,400 --> 00:06:43,310
in square root of t by choosing to resample things

138
00:06:43,310 --> 00:06:45,360
like-- excuse me, to rescale things.

139
00:06:45,360 --> 00:06:48,680
So that if we want we can think of the errors or the deviations

140
00:06:48,680 --> 00:06:51,650
appropriately scaled by powers of t

141
00:06:51,650 --> 00:06:55,670
so that they are drawn from a standard and 0, 1 distribution,

142
00:06:55,670 --> 00:06:58,490
if we'd like to apply that in a hypothesis testing case,

143
00:06:58,490 --> 00:07:00,770
and we want to take a look at usual confidence,

144
00:07:00,770 --> 00:07:04,600
or significant levels like plus or minus 2.

145
00:07:04,600 --> 00:07:07,900
Now, when we observe autocovariance

146
00:07:07,900 --> 00:07:10,840
what we're looking for is the direction

147
00:07:10,840 --> 00:07:14,950
and the magnitude of departures from a random walk.

148
00:07:14,950 --> 00:07:17,710
And we'd like to know how large the correlation is

149
00:07:17,710 --> 00:07:19,490
from one period to the next.

150
00:07:19,490 --> 00:07:23,590
So serial correlation is a typical analytic and statistic

151
00:07:23,590 --> 00:07:26,170
that we'll run when we get a time series of returns.

152
00:07:26,170 --> 00:07:27,850
It's an important attribute.

153
00:07:27,850 --> 00:07:30,160
After we've computed the mean and the variance,

154
00:07:30,160 --> 00:07:32,230
one of the next things we'll almost always want

155
00:07:32,230 --> 00:07:35,200
to take a look at to characterize what's going on

156
00:07:35,200 --> 00:07:38,690
is the autocorrelation or autocovariance functions.

157
00:07:38,690 --> 00:07:43,390
So in our example of two time steps for lag 1,

158
00:07:43,390 --> 00:07:44,450
we compute this.

159
00:07:44,450 --> 00:07:45,830
We have this expression.

160
00:07:45,830 --> 00:07:49,570
And what we can do is we can compute a variance ratio where

161
00:07:49,570 --> 00:07:52,780
we can compare our variance over two times steps

162
00:07:52,780 --> 00:07:56,410
to what we would have expected had it been a random walk.

163
00:07:56,410 --> 00:07:59,410
This would be 1 under the hypothesis

164
00:07:59,410 --> 00:08:00,970
that it is a random walk.

165
00:08:00,970 --> 00:08:04,210
And deviations from 1 means we have deviations

166
00:08:04,210 --> 00:08:06,250
from random walk behavior.

167
00:08:06,250 --> 00:08:09,640
If it's greater than 1 we have positive serial correlation.

168
00:08:09,640 --> 00:08:12,850
If it's less than 1 we have negative serial correlation.

169
00:08:12,850 --> 00:08:15,250
And we can generalize this to higher steps,

170
00:08:15,250 --> 00:08:17,860
and we can get an expression we can

171
00:08:17,860 --> 00:08:19,510
show with a little bit of algebra

172
00:08:19,510 --> 00:08:22,780
that we can express things as a sum, weighted sum

173
00:08:22,780 --> 00:08:26,290
of the different autocorrelation functions at different

174
00:08:26,290 --> 00:08:31,310
lags all the way up to value q, if we're looking at aggregating

175
00:08:31,310 --> 00:08:33,620
q steps at a time.

176
00:08:33,620 --> 00:08:35,590
So we'll typically look at all lags.

177
00:08:35,590 --> 00:08:38,570
Usually the first one is the one most of interest.

178
00:08:38,570 --> 00:08:42,750
But in principle we can look at any lag at any separation.

