0
00:00:00,000 --> 00:00:01,320


1
00:00:01,320 --> 00:00:03,270
PROFESSOR: Estimating the parameters of model

2
00:00:03,270 --> 00:00:05,460
is pretty easy and relatively straightforward

3
00:00:05,460 --> 00:00:07,260
when we're given this data set.

4
00:00:07,260 --> 00:00:11,670
Finding the right model or choosing among a set of models

5
00:00:11,670 --> 00:00:12,750
can be much harder.

6
00:00:12,750 --> 00:00:14,965
And there might not even be a right answer.

7
00:00:14,965 --> 00:00:16,590
So let's take a look at a few examples.

8
00:00:16,590 --> 00:00:19,300
We're going to start with the Monte Carlo example.

9
00:00:19,300 --> 00:00:24,330
And the reason is that it helps to know that your data actually

10
00:00:24,330 --> 00:00:25,420
comes from a model.

11
00:00:25,420 --> 00:00:28,380
So we'll start from a case that has a known answer.

12
00:00:28,380 --> 00:00:31,470
That way, we'll have some confidence when we start

13
00:00:31,470 --> 00:00:33,210
looking at other data sets.

14
00:00:33,210 --> 00:00:34,020
It's tough.

15
00:00:34,020 --> 00:00:37,200
And there may not always be a clear decision

16
00:00:37,200 --> 00:00:37,950
that you can make.

17
00:00:37,950 --> 00:00:41,160
And it requires judgment-- not only in selecting the model,

18
00:00:41,160 --> 00:00:44,370
but in understanding what the practical consequences are

19
00:00:44,370 --> 00:00:45,670
of having the wrong model.

20
00:00:45,670 --> 00:00:47,520
Is it just a good approximation?

21
00:00:47,520 --> 00:00:49,920
Or is it missing some essential components

22
00:00:49,920 --> 00:00:52,470
of the dynamics of the system and data generating

23
00:00:52,470 --> 00:00:54,180
process that you want to study?

24
00:00:54,180 --> 00:00:55,980
There's no easy way to know.

25
00:00:55,980 --> 00:00:58,590
And you need to constantly be on guard

26
00:00:58,590 --> 00:01:00,330
for things that might go wrong, and keep

27
00:01:00,330 --> 00:01:02,880
going back and refining your techniques.

28
00:01:02,880 --> 00:01:05,260
So let's start with an AR(2) example.

29
00:01:05,260 --> 00:01:07,860
Now remember that autoregressive models

30
00:01:07,860 --> 00:01:10,320
have the property that they do have

31
00:01:10,320 --> 00:01:13,330
correlations-- autocorrelations-- across time.

32
00:01:13,330 --> 00:01:16,110
And typically those autocorrelation functions

33
00:01:16,110 --> 00:01:20,790
drop off over time, but they exist at all lags.

34
00:01:20,790 --> 00:01:22,920
This is in contrast with MA models

35
00:01:22,920 --> 00:01:27,150
that are also defined recursively.

36
00:01:27,150 --> 00:01:30,600
But they're defined recursively in terms of the innovations

37
00:01:30,600 --> 00:01:32,260
that are localized in time.

38
00:01:32,260 --> 00:01:36,780
So at a sufficient separation in any given MA(q) model,

39
00:01:36,780 --> 00:01:38,550
the autocorrelation will go to zero.

40
00:01:38,550 --> 00:01:42,160
So that already helps us differentiate between the two.

41
00:01:42,160 --> 00:01:43,590
So let's look at our example.

42
00:01:43,590 --> 00:01:45,450
I'll generate some data here.

43
00:01:45,450 --> 00:01:47,280
You can download the notebook-- in fact,

44
00:01:47,280 --> 00:01:49,870
pause the video and do this now if you'd like--

45
00:01:49,870 --> 00:01:52,650
so you can follow along and tweak the parameters

46
00:01:52,650 --> 00:01:54,180
and try your own examples.

47
00:01:54,180 --> 00:01:57,000
Obviously the random numbers you generate will be different.

48
00:01:57,000 --> 00:01:59,670
But if you'd like to see the same random numbers, actually,

49
00:01:59,670 --> 00:02:03,870
why don't we set this up so that we

50
00:02:03,870 --> 00:02:07,860
can look at the same random numbers together.

51
00:02:07,860 --> 00:02:11,760
I'm going to add a command in our notebook

52
00:02:11,760 --> 00:02:15,090
which will make sure that we get the same random numbers

53
00:02:15,090 --> 00:02:21,290
by setting the seed 2021.

54
00:02:21,290 --> 00:02:25,820
Now, your computers may come up with a different set

55
00:02:25,820 --> 00:02:26,570
of random numbers.

56
00:02:26,570 --> 00:02:29,930
What this guarantees is, within a given setting,

57
00:02:29,930 --> 00:02:32,790
you will get on each one the same random numbers.

58
00:02:32,790 --> 00:02:35,660
So obviously in actual applications,

59
00:02:35,660 --> 00:02:37,488
this is the last thing you want to do.

60
00:02:37,488 --> 00:02:39,530
Because you want the random numbers to be random.

61
00:02:39,530 --> 00:02:42,390
Not just a single draw of random numbers to be reused.

62
00:02:42,390 --> 00:02:45,090
But for debugging purposes you might find it helpful.

63
00:02:45,090 --> 00:02:48,693
So you can put this in, comment it out or delete it afterwards.

64
00:02:48,693 --> 00:02:50,360
So here are the parameters of the model.

65
00:02:50,360 --> 00:02:52,940
We've got c_0 is 0.001.

66
00:02:52,940 --> 00:02:54,410
So it's a small intercept.

67
00:02:54,410 --> 00:02:55,790
It's close to zero.

68
00:02:55,790 --> 00:02:59,030
C_1, the first autoregressive parameter

69
00:02:59,030 --> 00:03:02,090
that is the coefficient of the lag 1 observation,

70
00:03:02,090 --> 00:03:03,860
is going to be minus 0.1.

71
00:03:03,860 --> 00:03:06,200
And c_2 is going to be larger and opposite sign.

72
00:03:06,200 --> 00:03:07,520
0.4.

73
00:03:07,520 --> 00:03:10,310
So let's take 1,000 times steps.

74
00:03:10,310 --> 00:03:12,650
We'll organize them in a column vector

75
00:03:12,650 --> 00:03:14,600
and set this up so that it generalizes

76
00:03:14,600 --> 00:03:16,850
for the case where you want to do an ensemble of paths

77
00:03:16,850 --> 00:03:17,630
together.

78
00:03:17,630 --> 00:03:19,277
But we'll run this forward.

79
00:03:19,277 --> 00:03:21,110
Now, one thing we need to be careful of when

80
00:03:21,110 --> 00:03:25,160
we start with time series models that are simulated--

81
00:03:25,160 --> 00:03:27,830
they obviously can't go infinitely far in the past.

82
00:03:27,830 --> 00:03:31,250
So we usually bootstrap them by setting the initial values off

83
00:03:31,250 --> 00:03:32,120
into zero.

84
00:03:32,120 --> 00:03:35,150
And we do need to make sure that our results don't

85
00:03:35,150 --> 00:03:35,990
depend on that.

86
00:03:35,990 --> 00:03:37,550
Those need to be a good approximation

87
00:03:37,550 --> 00:03:39,000
for larger values.

88
00:03:39,000 --> 00:03:42,050
So that's why I've initialized my variable r

89
00:03:42,050 --> 00:03:44,090
to be full of zeros, and I've started

90
00:03:44,090 --> 00:03:46,340
my recursion at time step 3.

91
00:03:46,340 --> 00:03:49,560
Because it needs to go back to previous time steps.

92
00:03:49,560 --> 00:03:52,040
So if we run this-- so we can see

93
00:03:52,040 --> 00:03:59,750
that I've defined a set of space placeholder for my variables.

94
00:03:59,750 --> 00:04:04,380
And I could set that to be plus 1 if I were doing market data.

95
00:04:04,380 --> 00:04:06,860
So I have a time 0, but here it's all simulated.

96
00:04:06,860 --> 00:04:07,790
It doesn't matter.

97
00:04:07,790 --> 00:04:09,690
I pick a bunch of random numbers.

98
00:04:09,690 --> 00:04:11,210
Which, in this case, I'm choosing

99
00:04:11,210 --> 00:04:14,360
from a N (0,1) distribution.

100
00:04:14,360 --> 00:04:15,740
These are normally distributed.

101
00:04:15,740 --> 00:04:17,839
And then I have a for loop where this is

102
00:04:17,839 --> 00:04:19,760
the basic line of my recursion.

103
00:04:19,760 --> 00:04:22,250
So notice I do need to iterate it forward.

104
00:04:22,250 --> 00:04:26,540
I can't do this with some clever algebraic tricks

105
00:04:26,540 --> 00:04:29,030
because I need to know what the values are at time t

106
00:04:29,030 --> 00:04:32,130
to know what they are for the next time step at t equals 1.

107
00:04:32,130 --> 00:04:35,270
So let's run this chunk of code, plot the results

108
00:04:35,270 --> 00:04:38,370
and see what we get.

109
00:04:38,370 --> 00:04:42,120
So the first graph that we have is going

110
00:04:42,120 --> 00:04:45,180
to be the AR(2) sample path.

111
00:04:45,180 --> 00:04:46,590
So it wanders.

112
00:04:46,590 --> 00:04:47,340
It looks random.

113
00:04:47,340 --> 00:04:49,980
Can you tell that it's autoregressive from looking

114
00:04:49,980 --> 00:04:50,730
at it?

115
00:04:50,730 --> 00:04:52,660
I can't.

116
00:04:52,660 --> 00:04:57,130
Let's look at the autocorrelation function.

117
00:04:57,130 --> 00:04:59,530
Well, here we see we're getting out what

118
00:04:59,530 --> 00:05:01,390
we expected when we put it in.

119
00:05:01,390 --> 00:05:05,510
That is, the first coefficient is nothing to do with c_0.

120
00:05:05,510 --> 00:05:07,060
That's normalized to equal to 1.

121
00:05:07,060 --> 00:05:10,210
That's the correlation of the series with itself.

122
00:05:10,210 --> 00:05:13,330
At lag 1 we have a negative number.

123
00:05:13,330 --> 00:05:15,340
And at lag 2 we have a positive number.

124
00:05:15,340 --> 00:05:17,170
And all the others look like they're

125
00:05:17,170 --> 00:05:19,850
at the limits of statistical significance.

126
00:05:19,850 --> 00:05:22,780
So that's about what we would have expected.

127
00:05:22,780 --> 00:05:24,700
Here's the partial ACF.

128
00:05:24,700 --> 00:05:27,040
Now remember what the partial ACF does.

129
00:05:27,040 --> 00:05:29,740
What it does is, it fits-- for each lag--

130
00:05:29,740 --> 00:05:34,570
it fits an autoregressive model of that number of terms.

131
00:05:34,570 --> 00:05:37,600
And the partial autocorrelation coefficient

132
00:05:37,600 --> 00:05:40,760
is the coefficient of the highest lag term.

133
00:05:40,760 --> 00:05:42,850
So what we would expect is that we're

134
00:05:42,850 --> 00:05:46,810
going to get non-zero results for each coefficient

135
00:05:46,810 --> 00:05:48,640
up to the order of our model.

136
00:05:48,640 --> 00:05:52,600
In an AR(p) model for the first p terms, in an AR(2) model

137
00:05:52,600 --> 00:05:53,980
for the first two terms.

138
00:05:53,980 --> 00:05:56,860
And everything above that should be close to zero.

139
00:05:56,860 --> 00:05:59,120
And that's exactly what we see here.

140
00:05:59,120 --> 00:06:01,750
So this would give us confidence that what we're looking at

141
00:06:01,750 --> 00:06:04,840
is an AR(2) model if we didn't know what the data was.

142
00:06:04,840 --> 00:06:08,840
Now let's go and remove this seed parameter and rerun it.

143
00:06:08,840 --> 00:06:11,540
Let's do a new set of random numbers.

144
00:06:11,540 --> 00:06:14,960
So I will run this chunk again.

145
00:06:14,960 --> 00:06:16,520
And let's take a look.

146
00:06:16,520 --> 00:06:19,170
This is a different process.

147
00:06:19,170 --> 00:06:22,530
We'll move over here, click on the second box.

148
00:06:22,530 --> 00:06:25,140
Here are its autocorrelation coefficients.

149
00:06:25,140 --> 00:06:29,490
You notice that the third and fourth are still showing up.

150
00:06:29,490 --> 00:06:35,400
But we do see the expected alternating signs and decay.

151
00:06:35,400 --> 00:06:39,360
And when we look at the partial ACF,

152
00:06:39,360 --> 00:06:47,990
we, again, see the lag 1 and 2 coefficients looking large,

153
00:06:47,990 --> 00:06:49,340
and all the others being small.

154
00:06:49,340 --> 00:06:51,560
And, of course, this plot out 30.

155
00:06:51,560 --> 00:06:55,820
And for 30 things to within 5% significance,

156
00:06:55,820 --> 00:06:57,440
we might expect a couple to be up.

157
00:06:57,440 --> 00:06:59,280
So these are things to do iteratively.

158
00:06:59,280 --> 00:07:03,050
If you see something suspicious, go dig in and investigate

159
00:07:03,050 --> 00:07:06,380
a little bit further.

160
00:07:06,380 --> 00:07:07,490
How do we estimate?

161
00:07:07,490 --> 00:07:11,120
So suppose we've decided, great, that data set is

162
00:07:11,120 --> 00:07:13,310
generated from an AR(2) model.

163
00:07:13,310 --> 00:07:14,330
It's looking good.

164
00:07:14,330 --> 00:07:17,310
Let's find the parameters of the AR(2) model.

165
00:07:17,310 --> 00:07:20,180
So once we settle on a model, really what we're doing

166
00:07:20,180 --> 00:07:23,412
is applying standard estimation techniques.

167
00:07:23,412 --> 00:07:24,620
There's nothing very special.

168
00:07:24,620 --> 00:07:27,510
In R there are some functions that help streamline it.

169
00:07:27,510 --> 00:07:31,160
But the basic idea is that we can do linear regression

170
00:07:31,160 --> 00:07:32,090
techniques.

171
00:07:32,090 --> 00:07:34,660
The only difference is that the variables

172
00:07:34,660 --> 00:07:36,410
on the right-hand side that normally would

173
00:07:36,410 --> 00:07:38,240
be independent variables are just

174
00:07:38,240 --> 00:07:42,000
lag versions of the variables on the left-hand side.

175
00:07:42,000 --> 00:07:46,520
So we can use ordinary least squares, maximum likelihood

176
00:07:46,520 --> 00:07:48,740
estimation, usual ways.

177
00:07:48,740 --> 00:07:51,560
And we can do that if we take the series

178
00:07:51,560 --> 00:07:53,480
and create lagged versions of it.

179
00:07:53,480 --> 00:07:57,260
Or R happens to provide-- and other languages do, too--

180
00:07:57,260 --> 00:08:00,630
packages that have some of these functions already built in.

181
00:08:00,630 --> 00:08:02,870
So let's take a look.

182
00:08:02,870 --> 00:08:04,280
In this case, let's define.

183
00:08:04,280 --> 00:08:05,630
Let's first write it--

184
00:08:05,630 --> 00:08:07,020
we'll do it in two methods.

185
00:08:07,020 --> 00:08:09,500
First we'll write it as an ordinary least squares.

186
00:08:09,500 --> 00:08:12,830
And then we'll do it using an special case

187
00:08:12,830 --> 00:08:15,380
of the built-in ARIMA function, where

188
00:08:15,380 --> 00:08:17,970
we'll look at the AR(2) as a special case of that.

189
00:08:17,970 --> 00:08:22,250
So the first thing is, I can define my variables.

190
00:08:22,250 --> 00:08:25,730
y is an independent variable in x1 and x2.

191
00:08:25,730 --> 00:08:27,710
Excuse me, y is a dependent variable.

192
00:08:27,710 --> 00:08:29,630
x1 and x2 is independent variables.

193
00:08:29,630 --> 00:08:31,430
But, of course, there are all from the time

194
00:08:31,430 --> 00:08:34,669
series of r, which we generated synthetically.

195
00:08:34,669 --> 00:08:36,530
Notice the time offset.

196
00:08:36,530 --> 00:08:40,130
Because we're looking at the correlation series with itself

197
00:08:40,130 --> 00:08:42,530
as the lags move, I'm going to end up

198
00:08:42,530 --> 00:08:45,860
with some non-matching points-- either at the beginning

199
00:08:45,860 --> 00:08:46,520
or at the end.

200
00:08:46,520 --> 00:08:48,312
And I need to trim those off so that I have

201
00:08:48,312 --> 00:08:49,850
data series of equal length.

202
00:08:49,850 --> 00:08:53,340
If the number of points you trim is significant,

203
00:08:53,340 --> 00:08:56,090
then your data series is too short.

204
00:08:56,090 --> 00:08:57,650
Or you're looking at--

205
00:08:57,650 --> 00:09:00,060
this is not a good model for what you want to be doing,

206
00:09:00,060 --> 00:09:01,220
if it's too sensitive.

207
00:09:01,220 --> 00:09:03,120
So the two ways we can look at it

208
00:09:03,120 --> 00:09:06,320
are we can run in R the lm function.

209
00:09:06,320 --> 00:09:07,880
Builds a linear model.

210
00:09:07,880 --> 00:09:09,410
It just runs linear regression.

211
00:09:09,410 --> 00:09:10,910
And you can do this in any language.

212
00:09:10,910 --> 00:09:11,868
You can do it in Excel.

213
00:09:11,868 --> 00:09:16,550
Alternatively, we can run the ARIMA function in R

214
00:09:16,550 --> 00:09:20,480
and fit this to a series where the order is

215
00:09:20,480 --> 00:09:26,480
given by the first parameter is the autoregressive part.

216
00:09:26,480 --> 00:09:28,310
The second is an integration level.

217
00:09:28,310 --> 00:09:30,750
And the third one is moving average part.

218
00:09:30,750 --> 00:09:33,400
So let's take a look and see what we get.

219
00:09:33,400 --> 00:09:36,230
So if we fit this by ordinarily least squares,

220
00:09:36,230 --> 00:09:38,510
we get the usual results in r.

221
00:09:38,510 --> 00:09:40,400
So we have an intercept.

222
00:09:40,400 --> 00:09:42,590
We have an x1 and an x2.

223
00:09:42,590 --> 00:09:45,470
We see that the intercept is close to zero.

224
00:09:45,470 --> 00:09:51,980
It's 0.034 with the standard error of 0.032.

225
00:09:51,980 --> 00:09:54,110
So that's close to zero.

226
00:09:54,110 --> 00:09:57,770
We didn't exactly pick out our carefully set c_0.

227
00:09:57,770 --> 00:10:02,520
But the x1 and x2 values are within the standard errors.

228
00:10:02,520 --> 00:10:04,280
So for that given the data series.

229
00:10:04,280 --> 00:10:07,460
That is, we have minus 0.07.

230
00:10:07,460 --> 00:10:10,340
And there's an approximation to minus 0.1.

231
00:10:10,340 --> 00:10:13,550
And we have, for the second coefficient,

232
00:10:13,550 --> 00:10:18,830
we've got plus 0.41, 0.42, no, 0.41.

233
00:10:18,830 --> 00:10:21,710
It's an approximation 0.4.

234
00:10:21,710 --> 00:10:24,650
So we got out what we expected going in.

235
00:10:24,650 --> 00:10:27,110
And we can take a look at the significance

236
00:10:27,110 --> 00:10:30,210
of the different values.

237
00:10:30,210 --> 00:10:34,620
Now, if we run this second method, this ARIMA function,

238
00:10:34,620 --> 00:10:36,465
we also get the coefficients here.

239
00:10:36,465 --> 00:10:38,090
It's laid out a little bit differently,

240
00:10:38,090 --> 00:10:39,382
but these are the same numbers.

241
00:10:39,382 --> 00:10:41,990
And it's just to show you that this is dressed up.

242
00:10:41,990 --> 00:10:45,900
It's really just doing an ordinary regression.

243
00:10:45,900 --> 00:10:48,567
So that's where we have the order correct.

244
00:10:48,567 --> 00:10:49,650
And we fit the parameters.

245
00:10:49,650 --> 00:10:52,070
So we decide what the model is.

246
00:10:52,070 --> 00:10:55,820
In this case, we generated the data, then we set that aside.

247
00:10:55,820 --> 00:10:59,060
We ran the data, we identified the order.

248
00:10:59,060 --> 00:11:01,160
We said that looks like it's order 2.

249
00:11:01,160 --> 00:11:04,370
We ran a fit to AR(2) model.

250
00:11:04,370 --> 00:11:06,800
We have estimates for the coefficients.

251
00:11:06,800 --> 00:11:09,980
They're noisy estimates because we have some noisy data,

252
00:11:09,980 --> 00:11:13,820
but then we can proceed with that estimated model

253
00:11:13,820 --> 00:11:15,660
with that set of parameters.

254
00:11:15,660 --> 00:11:17,390
Now, what if we got the order incorrect?

255
00:11:17,390 --> 00:11:19,100
That is, what if we thought--

256
00:11:19,100 --> 00:11:23,690
suppose there had been a spike, just a random jump, at order 5.

257
00:11:23,690 --> 00:11:26,150
And we said, ah, we've got an AR(5) model.

258
00:11:26,150 --> 00:11:28,190
Well, let's try running that.

259
00:11:28,190 --> 00:11:33,110
And in that case we find a set of coefficients here.

260
00:11:33,110 --> 00:11:36,780
And we can, again, take a look at what the estimates are.

261
00:11:36,780 --> 00:11:39,140
And we see that the standard errors show us

262
00:11:39,140 --> 00:11:41,810
that the errors are much larger than the coefficients

263
00:11:41,810 --> 00:11:43,670
themselves for the higher order terms.

264
00:11:43,670 --> 00:11:45,350
For AR(4) and AR(5).

265
00:11:45,350 --> 00:11:49,050
And even the third term is not statistically significant.

266
00:11:49,050 --> 00:11:52,100
So if we go too far, we can pull back

267
00:11:52,100 --> 00:11:56,960
and refit the model and retune for what we're doing.

268
00:11:56,960 --> 00:11:58,790
Try it yourself on some other data.

269
00:11:58,790 --> 00:12:01,220
See how well the quality of the estimates

270
00:12:01,220 --> 00:12:03,290
depends on the length of the series.

271
00:12:03,290 --> 00:12:05,150
That we can then turn around and think

272
00:12:05,150 --> 00:12:07,790
about how much data is required to get a good

273
00:12:07,790 --> 00:12:10,190
handle on the models.

274
00:12:10,190 --> 00:12:13,050
Let's take a look at this for an MA(2) model.

275
00:12:13,050 --> 00:12:16,280
So here is a model, which is MA(2).

276
00:12:16,280 --> 00:12:19,250
So it's got two lagged parameters.

277
00:12:19,250 --> 00:12:22,130
But those lag parameters this time are

278
00:12:22,130 --> 00:12:26,480
the z's on the right-hand side, not the r's.

279
00:12:26,480 --> 00:12:28,580
Not our random variable that we're

280
00:12:28,580 --> 00:12:30,440
observing, but the innovations.

281
00:12:30,440 --> 00:12:33,000
The shocks that occur in each time period.

282
00:12:33,000 --> 00:12:34,940
So I need to bootstrap this a little bit.

283
00:12:34,940 --> 00:12:37,100
Because at the beginning, again, we're

284
00:12:37,100 --> 00:12:39,510
not going infinitely far back in the past.

285
00:12:39,510 --> 00:12:43,430
So we're going to do this with our first two innovations.

286
00:12:43,430 --> 00:12:45,860
And then from time 3 forward to the end of the series

287
00:12:45,860 --> 00:12:47,570
we'll run this recursively.

288
00:12:47,570 --> 00:12:51,110
So our recursive definition is terrific for programmability.

289
00:12:51,110 --> 00:12:56,730
And let's run this model with a set of parameters.

290
00:12:56,730 --> 00:13:00,000
So if we take a look at what we get.

291
00:13:00,000 --> 00:13:02,260
Here's an MA(2) sample path.

292
00:13:02,260 --> 00:13:05,580
Does it look different from an AR(2) sample path?

293
00:13:05,580 --> 00:13:06,490
I can't say so.

294
00:13:06,490 --> 00:13:09,690
It looks like a noisy bunch of data to me.

295
00:13:09,690 --> 00:13:11,910
We did pick a very large value for sigma

296
00:13:11,910 --> 00:13:15,130
in relative terms for financial applications.

297
00:13:15,130 --> 00:13:16,020
But that's OK.

298
00:13:16,020 --> 00:13:17,610
Finance things are noisy.

299
00:13:17,610 --> 00:13:20,430
And you can try scaling these to different sizes

300
00:13:20,430 --> 00:13:22,350
to see how they look.

301
00:13:22,350 --> 00:13:25,590
How about our ACF and PACF functions?

302
00:13:25,590 --> 00:13:27,780
Well, here remember for the MA(2),

303
00:13:27,780 --> 00:13:30,900
we expect the ACF function to tell us when to stop.

304
00:13:30,900 --> 00:13:34,230
That is, the autocorrelation functions are only going

305
00:13:34,230 --> 00:13:37,170
to exist through the value q.

306
00:13:37,170 --> 00:13:38,430
And we see that here.

307
00:13:38,430 --> 00:13:40,830
We see that we've got autocorrelation functions

308
00:13:40,830 --> 00:13:44,190
and autocovariances, non-varnishing, for lags 1

309
00:13:44,190 --> 00:13:44,730
and 2.

310
00:13:44,730 --> 00:13:46,470
And then everything drops off.

311
00:13:46,470 --> 00:13:48,360
So that as we would expect.

312
00:13:48,360 --> 00:13:51,780
If we look at the PACF, the partial autocorrelation

313
00:13:51,780 --> 00:13:54,540
function, that's not much help here.

314
00:13:54,540 --> 00:13:59,495
So if this isn't showing us much,

315
00:13:59,495 --> 00:14:02,630
the reason is not that we've applied it incorrectly.

316
00:14:02,630 --> 00:14:04,340
It's because it's not the right tool

317
00:14:04,340 --> 00:14:06,500
for doing this identification.

318
00:14:06,500 --> 00:14:09,448
So when we pick our tools, you can throw everything

319
00:14:09,448 --> 00:14:11,240
at every dime series you have and you might

320
00:14:11,240 --> 00:14:12,462
see some interesting stuff.

321
00:14:12,462 --> 00:14:14,420
But think about the question you want to answer

322
00:14:14,420 --> 00:14:17,270
and specifically what it is we're looking for.

323
00:14:17,270 --> 00:14:19,730
Suppose we now want to estimate the parameters.

324
00:14:19,730 --> 00:14:21,090
Let's do that this time.

325
00:14:21,090 --> 00:14:24,770
We can run in R. You can repeat the ordinary least squares.

326
00:14:24,770 --> 00:14:27,390
Or we'll do it this way, using this notation.

327
00:14:27,390 --> 00:14:32,750
So (0,0,2) means it's only a moving average model.

328
00:14:32,750 --> 00:14:35,780
And as we run this, we get our coefficients.

329
00:14:35,780 --> 00:14:38,570
And we see our intercept is consistent with zero.

330
00:14:38,570 --> 00:14:43,130
And our MA(1) and MA(2) are lag 1 and lag 2.

331
00:14:43,130 --> 00:14:45,560
Coefficients for the phi's in the right-hand side

332
00:14:45,560 --> 00:14:47,240
are given by the results here.

333
00:14:47,240 --> 00:14:49,190
And again, try it out for longer data

334
00:14:49,190 --> 00:14:50,900
sets, for shorter data sets.

335
00:14:50,900 --> 00:14:53,960
And then for some real-world data.

336
00:14:53,960 --> 00:14:55,920
Speaking of real-world data.

337
00:14:55,920 --> 00:14:57,230
It's much harder.

338
00:14:57,230 --> 00:14:59,360
First of all, it typically doesn't

339
00:14:59,360 --> 00:15:01,290
fit any particular model.

340
00:15:01,290 --> 00:15:02,870
So if we're going to pick a model

341
00:15:02,870 --> 00:15:04,520
we'd like to estimate something that

342
00:15:04,520 --> 00:15:09,360
captures the important features of the data.

343
00:15:09,360 --> 00:15:11,790
And we don't even know what those necessarily are.

344
00:15:11,790 --> 00:15:13,710
So let's take a look at a data set.

345
00:15:13,710 --> 00:15:15,270
In this case, we'll grab some data

346
00:15:15,270 --> 00:15:18,900
from Yahoo Finance using the tidyquant package.

347
00:15:18,900 --> 00:15:21,340
And we'll get it for Tootsie Roll stock.

348
00:15:21,340 --> 00:15:24,390
So if you haven't used tidyquant before,

349
00:15:24,390 --> 00:15:26,910
you can uncomment this line and install it.

350
00:15:26,910 --> 00:15:29,170
We load it with the library command.

351
00:15:29,170 --> 00:15:32,280
And we'll define a ticker, a start, and an end date.

352
00:15:32,280 --> 00:15:36,570
And then this tq_get function grabs data from Yahoo Finance.

353
00:15:36,570 --> 00:15:39,180
Or you can just load it from a flat file yourself.

354
00:15:39,180 --> 00:15:42,940
So what does the data look like over time?

355
00:15:42,940 --> 00:15:44,370
Let's take a look at a chart.

356
00:15:44,370 --> 00:15:46,680
So that looks pretty noisy, too.

357
00:15:46,680 --> 00:15:50,020
This is more data points than we had before.

358
00:15:50,020 --> 00:15:52,570
There's an upward drift, which is definitely significant.

359
00:15:52,570 --> 00:15:54,480
Notice I set the drift term close to zero

360
00:15:54,480 --> 00:15:57,390
in both of our previous two examples.

361
00:15:57,390 --> 00:15:58,750
But here's the data.

362
00:15:58,750 --> 00:16:02,790
Now let's take a look at the time series of returns

363
00:16:02,790 --> 00:16:05,280
by taking the difference of the logarithms

364
00:16:05,280 --> 00:16:07,110
of the successive prices.

365
00:16:07,110 --> 00:16:08,940
And here's the return series.

366
00:16:08,940 --> 00:16:12,040
Now, when we get data for a stock or an asset,

367
00:16:12,040 --> 00:16:15,450
usually what we do-- in fact, what I almost always do--

368
00:16:15,450 --> 00:16:16,800
is I'll take the price data.

369
00:16:16,800 --> 00:16:18,630
I will turn it into returns like this.

370
00:16:18,630 --> 00:16:22,350
I'll use adjusted prices so I account for corporate actions

371
00:16:22,350 --> 00:16:24,690
and splits and dividends and so on.

372
00:16:24,690 --> 00:16:26,250
I'll take the adjusted series.

373
00:16:26,250 --> 00:16:29,160
And what I want to know is, what are the basic descriptive

374
00:16:29,160 --> 00:16:30,490
statistics for it?

375
00:16:30,490 --> 00:16:32,130
Which we can do on any data set.

376
00:16:32,130 --> 00:16:33,000
What is the mean?

377
00:16:33,000 --> 00:16:34,410
What is the standard deviation?

378
00:16:34,410 --> 00:16:37,590
Or, in financial terms, if we annualize numbers,

379
00:16:37,590 --> 00:16:39,930
what's the return and what's the risk?

380
00:16:39,930 --> 00:16:41,190
What are the parameters?

381
00:16:41,190 --> 00:16:43,170
Now, one of the things-- so we can do that

382
00:16:43,170 --> 00:16:44,490
anyways for anything.

383
00:16:44,490 --> 00:16:47,325
Those are just descriptive statistics for a set of data.

384
00:16:47,325 --> 00:16:49,200
They don't have anything to do with models.

385
00:16:49,200 --> 00:16:53,280
But if, in addition, we think about the generalized random

386
00:16:53,280 --> 00:16:56,850
walk model and we wanted to fit the data to that,

387
00:16:56,850 --> 00:17:01,650
well, mu and sigma are given by the same formulas

388
00:17:01,650 --> 00:17:03,870
that we have for the descriptive statistics--

389
00:17:03,870 --> 00:17:05,829
the mean and the standard deviation.

390
00:17:05,829 --> 00:17:07,420
So we would be in great shape.

391
00:17:07,420 --> 00:17:08,700
So we could ask.

392
00:17:08,700 --> 00:17:12,970
Is the generalized random walk a good fit for this data?

393
00:17:12,970 --> 00:17:15,359
Well, the answer is no.

394
00:17:15,359 --> 00:17:17,849
But what do you think?

395
00:17:17,849 --> 00:17:19,260
What do you think the reason is?

396
00:17:19,260 --> 00:17:22,151
So one of the things that I look at is the feature--

397
00:17:22,151 --> 00:17:23,609
there are a couple of features that

398
00:17:23,609 --> 00:17:26,819
are important-- of the random walk model.

399
00:17:26,819 --> 00:17:30,450
One of them is the lack of serial correlation.

400
00:17:30,450 --> 00:17:35,700
So we ought to immediately take a look at a plot of the ACF.

401
00:17:35,700 --> 00:17:37,590
Because if it's a random walk, there's

402
00:17:37,590 --> 00:17:40,720
no information propagating from one time period to the next.

403
00:17:40,720 --> 00:17:42,420
But before that, even at the level

404
00:17:42,420 --> 00:17:45,420
of the descriptive statistics, even before we compute them,

405
00:17:45,420 --> 00:17:47,910
we can see visually here that this does not

406
00:17:47,910 --> 00:17:49,950
look like a stationary process.

407
00:17:49,950 --> 00:17:52,470
So remember that stationarity means

408
00:17:52,470 --> 00:17:57,480
that we have time independent probability distributions.

409
00:17:57,480 --> 00:18:00,760
And it can be a very difficult thing to nail down, exactly.

410
00:18:00,760 --> 00:18:04,230
But certainly in the case of the random walk model,

411
00:18:04,230 --> 00:18:08,490
one of the things that we know is that in a stationary model,

412
00:18:08,490 --> 00:18:13,570
if the probability distributions are the same,

413
00:18:13,570 --> 00:18:17,250
then that necessarily means that if we estimate parameters

414
00:18:17,250 --> 00:18:19,217
from different subsets of the data,

415
00:18:19,217 --> 00:18:20,550
we ought to get the same values.

416
00:18:20,550 --> 00:18:22,890
At least, to within sampling errors.

417
00:18:22,890 --> 00:18:25,920
That means that there ought to be a given mean and a given

418
00:18:25,920 --> 00:18:29,410
standard deviation that we could sample over time.

419
00:18:29,410 --> 00:18:32,610
Now, a model that looks like this, if I run a Monte Carlo to

420
00:18:32,610 --> 00:18:36,330
simulate data that has matching statistics of the Tootsie Roll

421
00:18:36,330 --> 00:18:38,440
data, it looks like this.

422
00:18:38,440 --> 00:18:41,850
This is a white noise process that has a constant mean--

423
00:18:41,850 --> 00:18:43,230
zero, in this case--

424
00:18:43,230 --> 00:18:47,210
and a standard deviation that is constant over time.

425
00:18:47,210 --> 00:18:50,100
So we see that the data is time varying.

426
00:18:50,100 --> 00:18:52,800
But the probability distribution is not.

427
00:18:52,800 --> 00:18:58,680
On the other hand, the data that we see here for Tootsie Roll

428
00:18:58,680 --> 00:19:02,620
just, without running any fancy statistics,

429
00:19:02,620 --> 00:19:04,500
we can see that it's very, very unlikely

430
00:19:04,500 --> 00:19:06,330
to come from a generalized random walk

431
00:19:06,330 --> 00:19:10,720
model because of the presence of extreme spikes and outliers.

432
00:19:10,720 --> 00:19:13,230
So depending on what assumptions we

433
00:19:13,230 --> 00:19:15,960
would make for the distribution, this

434
00:19:15,960 --> 00:19:21,000
is unlikely to come from a single model

435
00:19:21,000 --> 00:19:23,280
with fixed parameters of the type

436
00:19:23,280 --> 00:19:26,460
of the generalized random walk over the entire period.

437
00:19:26,460 --> 00:19:28,230
The model that might fit much better

438
00:19:28,230 --> 00:19:31,020
is something that combines many of the elements we have here,

439
00:19:31,020 --> 00:19:33,090
but I'm not going to go into in this video.

440
00:19:33,090 --> 00:19:35,550
Something that's a model, for example, that has a time

441
00:19:35,550 --> 00:19:38,310
varying volatility parameter that

442
00:19:38,310 --> 00:19:41,520
would explain the difference between this and this.

443
00:19:41,520 --> 00:19:44,550
So this data, you can take.

444
00:19:44,550 --> 00:19:46,530
And I encourage you to try it and fit it

445
00:19:46,530 --> 00:19:49,630
to each of the models and answer these questions.

446
00:19:49,630 --> 00:19:51,850
Which of the models is the best fit?

447
00:19:51,850 --> 00:19:55,230
If you're looking for a model of autoregressive

448
00:19:55,230 --> 00:19:58,750
moving average or ARIMA type model, which is the best one?

449
00:19:58,750 --> 00:20:00,780
And what would be the parameters in those case?

450
00:20:00,780 --> 00:20:02,880
And what's the significance of the parameters?

451
00:20:02,880 --> 00:20:07,820
And do you think that the time series is stationary?

